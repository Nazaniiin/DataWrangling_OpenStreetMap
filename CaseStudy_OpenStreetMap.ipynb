{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenStreetMap Data Wrangling Process "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Short Introduction on OpenStreetMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenStreetMap (OSM) foundation is building free and editable map of the world, enabling the development of freely-reusable geospatial data. The data from OpenStreetMap is being used by many applications such as GoogleMaps, Foursquare and Craigslist. \n",
    "\n",
    "OpenStreetMap data structure has four core elements: Nodes, Ways, Tags, and Relations\n",
    "\n",
    "- Nodes are points with a geographic position stored as lon (longitude) and lan (latitude). They are used to show points on the map such as points of interest.\n",
    "- Ways are ordered list of nodes, representing a polyline or a polyline if they make a closed loop. They are used to show streets, rivers, and area such as parks, lakes, etc.\n",
    "- Relations are ordered list of nodes, ways and relations (called 'members') where each members can have a 'role' (a string). They are used to show the relation between nodes and ways such as restrictions on roads.\n",
    "- Tags are pairs of Keys and Values. They are used to store metdata of the map such as address, type of building, or any sort of physical property. Tags are always attached to a node, way or relation and are not stand-alone elements.\n",
    "\n",
    "To look at the map, or download your area of interest, you can visit http://www.openstreetmap.org website. \n",
    "\n",
    "For more information you can check their wiki which includes all the necessary information and documentation:\n",
    "https://en.wikipedia.org/wiki/OpenStreetMap\n",
    "\n",
    "Users can add points on the map, create relations of the nodes and ways, and assign properties such as address or type to them. The data can be stored in OSM format, and can be access in different formats. For the purpose of this project, I use the OSM XML format.\n",
    "http://wiki.openstreetmap.org/wiki/OSM_XML\n",
    "\n",
    "In this project, I will work with the raw data from an area. Since the data is put by different users, I suppose it can be quite messy; therefore, I will use cleaning and auditing functions to make the data look clean for analysis. I will export the data into CSV format and use this format to create tables in my SQLite database. Then I run queries on the database to extract information such as number of nodes and ways, most contributing users, top points of interest, etc. I will conclude the project by discussing benefits as well as some anticipated problems in implementing the improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Area Chosen for Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, I chose San Francisco area in the US. I chose this area as it is a point of interest for me with its big IT corporations; also, it is a place I want to travel to one day. I decided to download the file locally to my machine.  \n",
    "https://mapzen.com/data/metro-extracts/metro/san-francisco_california/\n",
    "\n",
    "The 'metro extracts' will provide the map of the metropolian area (i.e. where you can find more elements to work with)\n",
    "\n",
    "The original file is about 1.01GB in size; however, I use a sample file about 50MB to perform my initial analysis on. Once I am satisfied with the code, I run it on the original file to create the CSV files for my database. \n",
    "\n",
    "The data analyzed in this Jupyter notebook is from the sample file to be able to show shorter results from my analysis. I have included all the functions here, as well as the function to create CSV files, in separate .py files in my repository. Using those you can run the code on the original file. \n",
    "https://github.com/Nazaniiin/OpenStreetMap_DataWrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Data a bit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start going through the data, find its problems and clean them. First, we'll take a look into the dataset and parse through using ElementTree and extract information such as different types of elements (nodes,ways,etc.) in the OSM XML file.\n",
    "\n",
    "Using ET.iterparse (i.e. iterative parsing) is efficient here since the original file is too large for processing the whole thing. So iterative parsing will parse the file as it builds it.  \n",
    "http://stackoverflow.com/questions/12792998/elementtree-iterparse-strategy\n",
    "\n",
    "In this code, I will iterate through different tags in the XML (nodes,ways,relations,member,...) and count them, put them in a dictionary with the key being the tag name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'member': 2530,\n",
      " 'nd': 281292,\n",
      " 'node': 235744,\n",
      " 'osm': 1,\n",
      " 'relation': 283,\n",
      " 'tag': 87071,\n",
      " 'way': 27558}\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "\n",
    "OSMFILE = '/Users/nazaninmirarab/Desktop/Data Science/P3/Project/Submission2/san-francisco_california_sample.osm'\n",
    "\n",
    "def count_tags(filename):\n",
    "    tags= {}\n",
    "    for event, elem in ET.iterparse(filename):\n",
    "        if elem.tag not in tags.keys():\n",
    "            tags[elem.tag] = 1\n",
    "        else:\n",
    "            tags[elem.tag] += 1\n",
    "    \n",
    "    pprint.pprint(tags)\n",
    "    \n",
    "count_tags(OSMFILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "I will do a bit more exploration on the data. In the OSM XML file, the 'tag' element has key-value pairs which contain information about different points (nodes) or ways in the map. I parse through this element using the following regular expressions:\n",
    "- lower -> ^([a-z]|_)*$ : This matches strings that contain only lower case characters. I start at the beginning of the strong and match between zero to unlimited times a character in range 'a' to 'z'. This regular expression also covers the underscore '_' character. \n",
    "\n",
    "- lower_colon -> ^([a-z]|_)*:([a-z]|_)*$ : This matches strings which contain lower case characters but also have the colon ':' character e.g. addr:street is one type of tag which specifies a street name. \n",
    "\n",
    "- problemchar -> [=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n] : This matches tags with problematic characters specified in the regex pattern. \n",
    "\n",
    "Take this section of the map as an example:\n",
    "\n",
    "    <node changeset=\"30175357\" id=\"358830340\" lat=\"37.6504905\" lon=\"-122.4896963\" timestamp=\"2015-04-12T22:43:37Z\" \n",
    "        uid=\"35667\" user=\"encleadus\" version=\"4\">\n",
    "\t\t<tag k=\"name\" v=\"Ocean Shore School\" />\n",
    "\t\t<tag k=\"phone\" v=\"+1 650 738 6650\" />\n",
    "\t\t<tag k=\"amenity\" v=\"school\" />\n",
    "\t\t<tag k=\"website\" v=\"http://www.oceanshoreschool.org/\" />\n",
    "\t\t<tag k=\"addr:city\" v=\"Pacifica\" />\n",
    "\t\t<tag k=\"addr:state\" v=\"CA\" />\n",
    "\t\t<tag k=\"addr:street\" v=\"Oceana Boulevard\" />\n",
    "\t\t<tag k=\"gnis:created\" v=\"04/06/1998\" />\n",
    "\t\t<tag k=\"addr:postcode\" v=\"94044\" />\n",
    "\t\t<tag k=\"gnis:state_id\" v=\"06\" />\n",
    "\t\t<tag k=\"gnis:county_id\" v=\"081\" />\n",
    "\t\t<tag k=\"gnis:feature_id\" v=\"1785657\" />\n",
    "\t\t<tag k=\"addr:housenumber\" v=\"411\" />\n",
    "\t</node>\n",
    "    \n",
    "This node tag has 13 tag elements inside it. There are multiple keys that have the ':' character in them, so they fall under the 'lower_colon' regular expression. keys like name, phone, and amenity will fall under the 'lower' regular expression. There are no problematic characters in this specific node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lower': 51867, 'lower_colon': 33909, 'other': 1281, 'problemchars': 14}\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "import re\n",
    "\n",
    "OSMFILE = '/Users/nazaninmirarab/Desktop/Data Science/P3/Project/Submission2/san-francisco_california_sample.osm'\n",
    "\n",
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "\n",
    "def key_type(element, keys):\n",
    "    if element.tag == \"tag\":\n",
    "        for tag in element.iter('tag'): #iterating through the tag element in the XML file\n",
    "            k = element.attrib['k'] #looking for the tag attribute 'k' which contains the keys\n",
    "            if re.search(lower, k):\n",
    "                keys['lower'] += 1\n",
    "            elif re.search(lower_colon, k):\n",
    "                keys['lower_colon'] += 1\n",
    "            elif re.search(problemchars, k):\n",
    "                keys['problemchars'] += 1\n",
    "            else:\n",
    "                keys['other'] += 1\n",
    "                \n",
    "    return keys\n",
    "\n",
    "def process_map(filename):\n",
    "    keys = {\"lower\": 0, \"lower_colon\": 0, \"problemchars\": 0, \"other\": 0}\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        keys = key_type(element, keys)\n",
    "\n",
    "    pprint.pprint(keys)\n",
    "    \n",
    "process_map(OSMFILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I now want to collect some information about the users contributed to the OpenStreetMap data for San Francisco area. I want to calculate the number of unique users. \n",
    "\n",
    "To find the users, we need to look through the attributes of the node, way and relation tags. The 'uid' attribute is what we need to count.\n",
    "\n",
    "    <node changeset=\"30175357\" id=\"358830340\" lat=\"37.6504905\" lon=\"-122.4896963\" timestamp=\"2015-04-12T22:43:37Z\" \n",
    "    uid=\"35667\" user=\"encleadus\" version=\"4\">\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1284\n"
     ]
    }
   ],
   "source": [
    "OSMFILE = '/Users/nazaninmirarab/Desktop/Data Science/P3/Project/Submission2/san-francisco_california_sample.osm'\n",
    "\n",
    "def process_map(filename):\n",
    "    users = set()\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        if element.tag == 'node' or element.tag == 'way' or element.tag == 'relation':\n",
    "                userid = element.attrib['uid']\n",
    "                users.add(userid)\n",
    "\n",
    "    print len(users)\n",
    "    \n",
    "process_map(OSMFILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auditing Street Names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are many users who are entering data in OpenStreetMap, the way they represent the formatting of streets can vary. For example, the street type 'Avenue' can be written in formats such as:\n",
    "- Avenue (starting with capital letter)\n",
    "- Ave\n",
    "- Ave.\n",
    "- avenue (starting with small letter)\n",
    "\n",
    "To be able to process the data, we need to make these street types uniform. In case we are later searching for specific Avenue names, we can do a quick search on all street types that have the word 'Avenue' in them and we can make sure that we are not missing anything with abrreviations of Avenue.\n",
    "\n",
    "For this part, we need to:\n",
    "- Regular Expression -> \\b\\S+\\.?$ : Have a regular expression that can extract a string which might or might not have the '.' character in it (e.g. Ave. has the '.' character). This regular expression matches letters without any white space (\\S) with zero to one '.' \n",
    "- Have a list of names that we expect the streets to have (e.g. Avenue, Street, Highway, ...)\n",
    "- Collect a list of all types of street names that do NOT match the ones in the expected list; and change those to one matching the expected list. For example, changing 'St.' to 'Street'. \n",
    "- Parse through the tags where they keys are equal to 'addr:street' and collect the value attribute of them. I will use the regular expression explained earlier to read the strings in the value attribute. \n",
    "For example:\n",
    "      <tag k=\"addr:street\" v=\"Oceana Blv.\" />\n",
    "\n",
    "- Make a dictionary with the values being the value attribute of the tags and keys being the street type found from the regular expression.\n",
    "- Create a list called 'mapping' and enter all the different varieties of street types found\n",
    "- Change all those street names in the mapping list to match the ones in the expected list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import pprint\n",
    "\n",
    "OSMFILE = \"/Users/nazaninmirarab/Desktop/Data Science/P3/Project/Submission2/san-francisco_california_sample.osm\"\n",
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "\n",
    "# the list of street types that we want to have\n",
    "expected = [\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\", \"Square\", \"Lane\", \"Road\", \n",
    "            \"Trail\", \"Parkway\", \"Commons\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'audit_street_type' function will get the list of street types and using the regular expression, compare them to the expected list. If they do not match the names in the expected list, it adds it to the street_types dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected:\n",
    "            street_types[street_type].add(street_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'is_street_name' function will get the elements in the file (i.e. the tag element) and return the attributes in that element for which their key is equal to 'addr:street'. \n",
    "\n",
    "The 'audit' funntion uses iterative parsing to go through the XML file, parse node and way elements, and iterate through their tag element. It will then call the 'audit_street_type' function to add the value attribute of the tag (i.e. the street name) to it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_street_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "\n",
    "def audit(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    street_types = defaultdict(set)\n",
    "    \n",
    "    #parses the XML file\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "        # iterate through the 'tag' element of node and way elements\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_street_name(tag):\n",
    "                    audit_street_type(street_types, tag.attrib['v'])\n",
    "    osm_file.close()\n",
    "    return street_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will call the 'audit' function and use pretty print to get a nice-looking output of the dictionary that has the names of the streets in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': set(['Upton St #A']),\n",
      " 'Alameda': set(['Alameda', 'The Alameda']),\n",
      " 'Alley': set(['Hodges Alley']),\n",
      " 'Ave': set(['San Pablo Ave', 'Tehama Ave']),\n",
      " 'Ave.': set(['Santa Cruz Ave.']),\n",
      " 'Avenie': set(['Garvin Avenie']),\n",
      " 'Blvd': set(['N California Blvd']),\n",
      " 'Broadway': set(['Broadway']),\n",
      " 'Building': set(['Ferry Building']),\n",
      " 'Center': set(['Bon Air Center', 'Westlake Center']),\n",
      " 'Circle': set(['Blossom Circle',\n",
      "                'Croydon Circle',\n",
      "                'Gloria Circle',\n",
      "                'Inner Circle',\n",
      "                'Wilson Circle']),\n",
      " 'Gardens': set(['Wildwood Gardens']),\n",
      " 'H': set(['Avenue H']),\n",
      " 'Highway': set(['Bayshore Highway', 'Great Highway', 'Shoreline Highway']),\n",
      " 'Hyde': set(['Hyde']),\n",
      " 'Las': set(['Alameda De Las']),\n",
      " 'Lugano': set(['Via Lugano']),\n",
      " 'Marina': set(['Pacific Marina']),\n",
      " 'Mason': set(['Fort Mason']),\n",
      " 'Ness': set(['Van Ness']),\n",
      " 'North': set(['Mission Bay Boulevard North']),\n",
      " 'Ora': set(['Avenue Del Ora']),\n",
      " 'Path': set(['Indian Rock Path', 'Mendocino Path']),\n",
      " 'Plaza': set(['El Cerrito Plaza', 'Lakeshore Plaza']),\n",
      " 'Plz': set(['Woodside Plz']),\n",
      " 'Real': set(['El Camino Real', 'West El Camino Real']),\n",
      " 'St': set(['Bell St', 'Delancey St']),\n",
      " 'Terrace': set(['Eagle Hill Terrace',\n",
      "                 'Grenard Terrace',\n",
      "                 'Hawthorne Terrace']),\n",
      " 'Way': set(['Abbott Way',\n",
      "             'Altamont Way',\n",
      "             'Bayridge Way',\n",
      "             'Bel Air Way',\n",
      "             'Berkeley Way',\n",
      "             'Bill Drake Way',\n",
      "             'Black Fox Way',\n",
      "             'Boulevard Way',\n",
      "             'Bristol Way',\n",
      "             'Buena Vista Way',\n",
      "             'California Way',\n",
      "             'Camberly Way',\n",
      "             'Cambridge Way',\n",
      "             'Cardinal Way',\n",
      "             'Channing Way',\n",
      "             'Chelsea Way',\n",
      "             'Cheshire Way',\n",
      "             'Devonshire Way',\n",
      "             'Dwight Way',\n",
      "             'Eastlake Way',\n",
      "             'Edgecliff Way',\n",
      "             'Glenn Way',\n",
      "             'Hacker Way',\n",
      "             'Hanna Way',\n",
      "             'Harbor Way',\n",
      "             'Highland Way',\n",
      "             'Kohoutek Way',\n",
      "             'Lancaster Way',\n",
      "             'Lenox Way',\n",
      "             'Lincoln Way',\n",
      "             'Marina Way',\n",
      "             'Martin Luther King Jr Way',\n",
      "             'Mcnulty Way',\n",
      "             'Midfield Way',\n",
      "             'Mills Way',\n",
      "             'Mitchell Way',\n",
      "             'Park Way',\n",
      "             'Piedmont Way',\n",
      "             'Ranleigh Way',\n",
      "             'Red Oak Way',\n",
      "             'Sanchez Way',\n",
      "             'Seminole Way',\n",
      "             'Shepard Way',\n",
      "             'Sterling Way',\n",
      "             'Sussex Way',\n",
      "             'Sylvan Way',\n",
      "             'West California Way',\n",
      "             'Windsor Way',\n",
      "             'Woodland Way']),\n",
      " 'Yolanda': set(['Corte Yolanda']),\n",
      " 'avenue': set(['Santa Cruz avenue'])}\n"
     ]
    }
   ],
   "source": [
    "street_types = audit(OSMFILE)\n",
    "\n",
    "pprint.pprint(dict(street_types))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "======================================================================================================================\n",
    "\n",
    "Going through the street name list, I will use it to update the 'mapping' list. In this list I mention the format of the street type that was found in the file (left) and specify to what format it needs to be changed (right).\n",
    "\n",
    "The dictionary containing the abbreviated street types do not cover all the different street types, but covers a comprehensive number of them. I go through this list and see which ones can be changed. For instance:\n",
    "- 'Alameda': set(['Alameda', 'The Alameda'\n",
    "\n",
    "is a name that do not need any changes; therefore, I leave it out. However, a list like:\n",
    "- 'St': set(['Bell St', 'Delancey St']\n",
    "\n",
    "can definately change from 'Bell St -> Bell Street'. So, I will add 'St' to the mapping and specify what I expect to get after updating it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The list of dictionaries, containing street types that need to be changed to match the expected list\n",
    "mapping = { \"St\": \"Street\",\n",
    "            \"St.\": \"Street\",\n",
    "            \"street\": \"Street\",\n",
    "            \"Ave\": \"Avenue\",\n",
    "            \"Ave.\": \"Avenue\",\n",
    "            \"AVE\": \"Avenue,\",\n",
    "            \"avenue\": \"Avenue\",\n",
    "            \"Rd.\": \"Road\",\n",
    "            \"Rd\": \"Road\",\n",
    "            \"road\": \"Road\",\n",
    "            \"Blvd\": \"Boulevard\",\n",
    "            \"Blvd.\": \"Boulevard\",\n",
    "            \"Blvd,\": \"Boulevard\",\n",
    "            \"boulevard\": \"Boulevard\",\n",
    "            \"broadway\": \"Broadway\",\n",
    "            \"square\": \"Square\",\n",
    "            \"way\": \"Way\",\n",
    "            \"Dr.\": \"Drive\",\n",
    "            \"Dr\": \"Drive\",\n",
    "            \"ct\": \"Court\",\n",
    "            \"Ct\": \"Court\",\n",
    "            \"court\": \"Court\",\n",
    "            \"Sq\": \"Square\",\n",
    "            \"square\": \"Square\",\n",
    "            \"cres\": \"Crescent\",\n",
    "            \"Cres\": \"Crescent\",\n",
    "            \"Ctr\": \"Center\",\n",
    "            \"Hwy\": \"Highway\",\n",
    "            \"hwy\": \"Highway\",\n",
    "            \"Ln\": \"Lane\",\n",
    "            \"Ln.\": \"Lane\",\n",
    "            \"parkway\": \"Parkway\"\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To match the expected list of street name and replace the abbreviated street types, I wrote a function that uses the mapping to do this conversion.\n",
    "\n",
    "I take the street name (e.g. N California Blvd) and split it at the space character. In case I could find a string that matches any in the mapping, I replace it with the format I have specified for it. When the function finds 'Blvd', it goes through mapping and map it to 'Boulevard', and the final street name will come out as 'N California Boulevard'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_name(name, mapping):\n",
    "    output = list()\n",
    "    parts = name.split(\" \")\n",
    "    for part in parts:\n",
    "        if part in mapping:\n",
    "            output.append(mapping[part])\n",
    "        else:\n",
    "            output.append(part)\n",
    "    return \" \".join(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a print to see how the changes have been applied. I iterate through the street_types from which collected different street types from the 'audit' function, and call the 'update_name' function to change the street type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upton St #A => Upton Street #A\n",
      "Via Lugano => Via Lugano\n",
      "Van Ness => Van Ness\n",
      "Devonshire Way => Devonshire Way\n",
      "Ranleigh Way => Ranleigh Way\n",
      "Piedmont Way => Piedmont Way\n",
      "Bel Air Way => Bel Air Way\n",
      "Berkeley Way => Berkeley Way\n",
      "Black Fox Way => Black Fox Way\n",
      "Windsor Way => Windsor Way\n",
      "Boulevard Way => Boulevard Way\n",
      "Sussex Way => Sussex Way\n",
      "Mitchell Way => Mitchell Way\n",
      "Eastlake Way => Eastlake Way\n",
      "Bayridge Way => Bayridge Way\n",
      "Cheshire Way => Cheshire Way\n",
      "Bristol Way => Bristol Way\n",
      "Martin Luther King Jr Way => Martin Luther King Jr Way\n",
      "Chelsea Way => Chelsea Way\n",
      "Mcnulty Way => Mcnulty Way\n",
      "Mills Way => Mills Way\n",
      "Dwight Way => Dwight Way\n",
      "Glenn Way => Glenn Way\n",
      "Camberly Way => Camberly Way\n",
      "Lincoln Way => Lincoln Way\n",
      "Sterling Way => Sterling Way\n",
      "Buena Vista Way => Buena Vista Way\n",
      "Shepard Way => Shepard Way\n",
      "Red Oak Way => Red Oak Way\n",
      "Park Way => Park Way\n",
      "Altamont Way => Altamont Way\n",
      "California Way => California Way\n",
      "Marina Way => Marina Way\n",
      "Bill Drake Way => Bill Drake Way\n",
      "Lancaster Way => Lancaster Way\n",
      "Hacker Way => Hacker Way\n",
      "Midfield Way => Midfield Way\n",
      "Hanna Way => Hanna Way\n",
      "Abbott Way => Abbott Way\n",
      "Harbor Way => Harbor Way\n",
      "Kohoutek Way => Kohoutek Way\n",
      "Sylvan Way => Sylvan Way\n",
      "Cambridge Way => Cambridge Way\n",
      "Channing Way => Channing Way\n",
      "Highland Way => Highland Way\n",
      "Seminole Way => Seminole Way\n",
      "Edgecliff Way => Edgecliff Way\n",
      "Sanchez Way => Sanchez Way\n",
      "Cardinal Way => Cardinal Way\n",
      "West California Way => West California Way\n",
      "Lenox Way => Lenox Way\n",
      "Woodland Way => Woodland Way\n",
      "Garvin Avenie => Garvin Avenie\n",
      "Inner Circle => Inner Circle\n",
      "Gloria Circle => Gloria Circle\n",
      "Blossom Circle => Blossom Circle\n",
      "Croydon Circle => Croydon Circle\n",
      "Wilson Circle => Wilson Circle\n",
      "Alameda => Alameda\n",
      "The Alameda => The Alameda\n",
      "Bayshore Highway => Bayshore Highway\n",
      "Great Highway => Great Highway\n",
      "Shoreline Highway => Shoreline Highway\n",
      "El Camino Real => El Camino Real\n",
      "West El Camino Real => West El Camino Real\n",
      "Mission Bay Boulevard North => Mission Bay Boulevard North\n",
      "Santa Cruz avenue => Santa Cruz Avenue\n",
      "Woodside Plz => Woodside Plz\n",
      "Corte Yolanda => Corte Yolanda\n",
      "Pacific Marina => Pacific Marina\n",
      "Indian Rock Path => Indian Rock Path\n",
      "Mendocino Path => Mendocino Path\n",
      "Avenue Del Ora => Avenue Del Ora\n",
      "Ferry Building => Ferry Building\n",
      "Santa Cruz Ave. => Santa Cruz Avenue\n",
      "Westlake Center => Westlake Center\n",
      "Bon Air Center => Bon Air Center\n",
      "Avenue H => Avenue H\n",
      "Lakeshore Plaza => Lakeshore Plaza\n",
      "El Cerrito Plaza => El Cerrito Plaza\n",
      "Bell St => Bell Street\n",
      "Delancey St => Delancey Street\n",
      "Tehama Ave => Tehama Avenue\n",
      "San Pablo Ave => San Pablo Avenue\n",
      "Fort Mason => Fort Mason\n",
      "Wildwood Gardens => Wildwood Gardens\n",
      "Hodges Alley => Hodges Alley\n",
      "Hyde => Hyde\n",
      "Grenard Terrace => Grenard Terrace\n",
      "Eagle Hill Terrace => Eagle Hill Terrace\n",
      "Hawthorne Terrace => Hawthorne Terrace\n",
      "N California Blvd => N California Boulevard\n",
      "Broadway => Broadway\n",
      "Alameda De Las => Alameda De Las\n"
     ]
    }
   ],
   "source": [
    "for st_type, ways in street_types.iteritems():\n",
    "        for name in ways:\n",
    "            better_name = update_name(name, mapping)\n",
    "            print name, \"=>\", better_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auditing Postcodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Postcodes are another inconsistent type of data that is entered into the map. The inconsistency is either in how they are represented (with the city abbreviation or without) or how long they are.\n",
    "\n",
    "The theory behind audting Postcodes is the same as auditing street names. \n",
    "\n",
    "I need to first check how the postcodes are being shown in the map. \n",
    "\n",
    "In the 'dicti' function, I create a dictionary where I can hold postcodes. The dictionary key will be the postcode itself and the dictionary value will be the number of times that postcode was repeated throughout the map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OSMFILE = '/Users/nazaninmirarab/Desktop/Data Science/P3/Project/Submission2/san-francisco_california_sample.osm'\n",
    "\n",
    "def dicti(data, item):\n",
    "    data[item] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'get_postcode' function will take the 'tag' element as an input and return the elements for which the keys are equal to 'addr:postcode' \n",
    "\n",
    "The 'audit' function, like the one for street names, parses the XML file and iterates through node and way elements. It extracts the value attribute (i.e. the postcode) and add it to the 'dicti' dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_postcode(elem):\n",
    "    return (elem.attrib['k'] == \"addr:postcode\")\n",
    "\n",
    "def audit(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    data = defaultdict(int)\n",
    "    # parsing the XML file\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "        \n",
    "        # iterating through node and way elements.\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if get_postcode(tag):\n",
    "                    dicti(data, tag.attrib['v'])\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will call the 'audit' function and print the output which should be a list of dictionaries of postcodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'90214': 1,\n",
      " '94002': 2,\n",
      " '94005': 1,\n",
      " '94010': 3,\n",
      " '94014': 1,\n",
      " '94015': 2,\n",
      " '94019': 1,\n",
      " '94025': 1,\n",
      " '94027': 1,\n",
      " '94044': 4,\n",
      " '94061': 9,\n",
      " '94063': 17,\n",
      " '94065': 4,\n",
      " '94066': 1,\n",
      " '94070': 1,\n",
      " '94080': 2,\n",
      " '94102': 8,\n",
      " '94103': 39,\n",
      " '94104': 4,\n",
      " '94105': 2,\n",
      " '94107': 13,\n",
      " '94108': 8,\n",
      " '94109': 20,\n",
      " '94110': 8,\n",
      " '94111': 2,\n",
      " '94112': 3,\n",
      " '94113': 4,\n",
      " '94114': 20,\n",
      " '94115': 3,\n",
      " '94116': 102,\n",
      " '94117': 60,\n",
      " '94118': 4,\n",
      " '94121': 12,\n",
      " '94121-3131': 1,\n",
      " '94122': 254,\n",
      " '94123': 10,\n",
      " '94124': 2,\n",
      " '94127': 32,\n",
      " '94128': 1,\n",
      " '94129': 1,\n",
      " '94131': 9,\n",
      " '94132': 3,\n",
      " '94133': 46,\n",
      " '94134': 1,\n",
      " '94143': 1,\n",
      " '94158': 2,\n",
      " '94166': 1,\n",
      " '94303': 1,\n",
      " '94402': 1,\n",
      " '94403': 3,\n",
      " '94404': 3,\n",
      " '94501': 5,\n",
      " '94507': 1,\n",
      " '94530': 5,\n",
      " '94536': 1,\n",
      " '94544': 2,\n",
      " '94545': 1,\n",
      " '94546': 1,\n",
      " '94549': 1,\n",
      " '94552': 1,\n",
      " '94556': 1,\n",
      " '94577': 5,\n",
      " '94578': 5,\n",
      " '94587': 16,\n",
      " '94596': 1,\n",
      " '94598': 3,\n",
      " '94601': 1,\n",
      " '94602': 4,\n",
      " '94606': 6,\n",
      " '94607': 5,\n",
      " '94608': 4,\n",
      " '94609': 2,\n",
      " '94610': 69,\n",
      " '94611': 144,\n",
      " '94612': 6,\n",
      " '94618': 6,\n",
      " '94619': 3,\n",
      " '94702': 1,\n",
      " '94703': 2,\n",
      " '94704': 4,\n",
      " '94706': 10,\n",
      " '94707': 1,\n",
      " '94804': 2,\n",
      " '94904': 1,\n",
      " '94941': 2,\n",
      " '94965': 2,\n",
      " '95498': 3}\n"
     ]
    }
   ],
   "source": [
    "postcodes = audit(OSMFILE)\n",
    "\n",
    "pprint.pprint(dict(postcodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "======================================================================================================================\n",
    "### Different Postcodes and Ways to Clean Them up\n",
    "The output shows that the postcodes are in these formats:\n",
    "- A 5-digit format (e.g. 12345)\n",
    "- A 5-digit format followed by more numbers after a hyphen (e.g. 12345-6789)\n",
    "\n",
    "After running the code on the original file, I also found out that there were postcodes which where:\n",
    "- A format with which the city name is mentioned in the beginning (e.g. CA 12345), or\n",
    "- A format shorter or longer than 5 digits (e.g. 515 or 134567)\n",
    "- A format that only has the 'CA' characters in them, with no digit\n",
    "\n",
    "To deal with the postcodes, I divide them into different categories:\n",
    "- First category include the ones:\n",
    "    - Where the length equals to 5 (e.g. 12345)\n",
    "    - Where the length is longer than 5, and they contain characters (like abbreviations of a city) (e.g. CA 12345)\n",
    "    \n",
    "- Second category include the ones:\n",
    "    - Where the length is longer than 5, and they are followed by a hyphen (e.g. 12345-6789)\n",
    "    \n",
    "- Third category include the ones:\n",
    "    - Where the length is longer than 5, but are not followed by any hyphen (e.g. 123456)\n",
    "    - Where the length is shorter than 5 (e.g. 1234, 515)\n",
    "    - Where the postcode equals to 'CA'\n",
    "    \n",
    "For the first category, I use a regular expression to extract only 5 digits from the pattern. This regex asserts position at start of the string ( ^ ) and matches any character that is NOT a digit ( \\D* ). The ( \\d{5} ) matches a digit exactly 5 times. In case the postcode starts with letters (e.g. CA 12345), it gives two groups of output: One is 'CA' and the other is '12345'. Depending on which one is needed, the preferred group can be chosen.  \n",
    "\n",
    "    ^\\D*(\\d{5}).*\n",
    "\n",
    "For the second category, I use another regular expression to extract the first 5 digits. This regex matches digits 5 times, is followed by a '-', and then matching digits exactly 4 times.\n",
    "\n",
    "    ^(\\d{5})-\\d{4}$\n",
    "    \n",
    "For the third category, having postcodes which have shorter or longer than 5-digit length, means that they are not valid. To clean them up, I replace those postcodes with '00000'. I use regular expressions to be able to find the ones that are exactly 6-digit long. \n",
    "\n",
    "    ^\\d{6}$ is to find postcodes that are exactly 6-digit long\n",
    "    \n",
    "\n",
    "Now I write the 'update_postcode' function. I use different conditions in the function to match the postcodes in the 3 categories I explained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_postcode(digit):\n",
    "    output = list()\n",
    "    \n",
    "    first_category = re.compile(r'^\\D*(\\d{5}$)', re.IGNORECASE)\n",
    "    \n",
    "    second_category = re.compile('^(\\d{5})-\\d{4}$')\n",
    "    \n",
    "    third_category = re.compile('^\\d{6}$')\n",
    "    \n",
    "    if re.search(first_category, digit):\n",
    "        new_digit = re.search(first_category, digit).group(1)\n",
    "        output.append(new_digit)\n",
    "        \n",
    "    elif re.search(second_category, digit):\n",
    "        new_digit = re.search(second_category, digit).group(1)\n",
    "        output.append(new_digit)\n",
    "    \n",
    "    elif re.search(third_category, digit):\n",
    "        third_output = third_category.search(digit)\n",
    "        new_digit = '00000'\n",
    "        output.append('00000')\n",
    "    \n",
    "    # this condition matches the third category for the other two types of postcodes\n",
    "    elif digit == 'CA' or len(digit) < 5:\n",
    "        new_digit = '00000'\n",
    "        output.append(new_digit)\n",
    "\n",
    "    return ', '.join(str(x) for x in output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will print the output after the changes are done to the postcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94121-3131 => 94121\n",
      "94404 => 94404\n",
      "94402 => 94402\n",
      "94403 => 94403\n",
      "94118 => 94118\n",
      "94611 => 94611\n",
      "94610 => 94610\n",
      "94612 => 94612\n",
      "94112 => 94112\n",
      "94113 => 94113\n",
      "94110 => 94110\n",
      "94111 => 94111\n",
      "94116 => 94116\n",
      "94618 => 94618\n",
      "94114 => 94114\n",
      "94115 => 94115\n",
      "90214 => 90214\n",
      "94577 => 94577\n",
      "94578 => 94578\n",
      "94124 => 94124\n",
      "94123 => 94123\n",
      "94122 => 94122\n",
      "94121 => 94121\n",
      "94129 => 94129\n",
      "94128 => 94128\n",
      "94606 => 94606\n",
      "94607 => 94607\n",
      "94602 => 94602\n",
      "94601 => 94601\n",
      "94044 => 94044\n",
      "94804 => 94804\n",
      "94608 => 94608\n",
      "94609 => 94609\n",
      "94965 => 94965\n",
      "94131 => 94131\n",
      "94132 => 94132\n",
      "94133 => 94133\n",
      "94134 => 94134\n",
      "94507 => 94507\n",
      "94501 => 94501\n",
      "94587 => 94587\n",
      "94303 => 94303\n",
      "95498 => 95498\n",
      "94904 => 94904\n",
      "94143 => 94143\n",
      "94027 => 94027\n",
      "94025 => 94025\n",
      "94596 => 94596\n",
      "94598 => 94598\n",
      "94619 => 94619\n",
      "94117 => 94117\n",
      "94015 => 94015\n",
      "94014 => 94014\n",
      "94010 => 94010\n",
      "94019 => 94019\n",
      "94158 => 94158\n",
      "94109 => 94109\n",
      "94108 => 94108\n",
      "94080 => 94080\n",
      "94536 => 94536\n",
      "94530 => 94530\n",
      "94005 => 94005\n",
      "94166 => 94166\n",
      "94104 => 94104\n",
      "94549 => 94549\n",
      "94545 => 94545\n",
      "94544 => 94544\n",
      "94546 => 94546\n",
      "94707 => 94707\n",
      "94706 => 94706\n",
      "94704 => 94704\n",
      "94703 => 94703\n",
      "94702 => 94702\n",
      "94070 => 94070\n",
      "94552 => 94552\n",
      "94556 => 94556\n",
      "94066 => 94066\n",
      "94127 => 94127\n",
      "94065 => 94065\n",
      "94063 => 94063\n",
      "94061 => 94061\n",
      "94103 => 94103\n",
      "94102 => 94102\n",
      "94105 => 94105\n",
      "94002 => 94002\n",
      "94107 => 94107\n",
      "94941 => 94941\n"
     ]
    }
   ],
   "source": [
    "for postcode, nums in postcodes.iteritems():\n",
    "    better_code = update_postcode(postcode)\n",
    "    print postcode, \"=>\", better_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data for the Database\n",
    "\n",
    "To load the data to the SQLite database, I need to transfer it from the XML file to CSV files. I create multiple CSV files, and later create the corresponding tables in my database based on them.\n",
    "\n",
    "The CSV files I want to have are:\n",
    "- Node\n",
    "- Node_tags\n",
    "- Way\n",
    "- Way_tags\n",
    "- Way_nodes\n",
    "\n",
    "Each of these CSV files contains different columns and stores data based on those columns. The columns used in the CSV files will be the table columns in the database. This is the schema:\n",
    "- NODE_FIELDS = ['id', 'lat', 'lon', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "- NODE_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "- WAY_FIELDS = ['id', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "- WAY_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "- WAY_NODES_FIELDS = ['id', 'node_id', 'position']\n",
    "\n",
    "To create these files, I will parse the 'node' and 'way' tags and extract the tags inside them. \n",
    "\n",
    "The 'shape_element' function takes as input an iterparse Element object and returns a dictionary. Depending on whether the element is 'node' or 'way', the dictionary looks different. \n",
    "\n",
    "Here's an example from the 'node' element in the XML file:\n",
    "\n",
    "    <node changeset=\"27772228\" id=\"358830414\" lat=\"37.6668652\" lon=\"-122.4895243\" timestamp=\"2014-12-29T09:43:14Z\" uid=\"14293\" user=\"KindredCoda\" version=\"2\">\n",
    "\t\t<tag k=\"ele\" v=\"118\" />\n",
    "\t\t<tag k=\"name\" v=\"Longview Park\" />\n",
    "\t\t<tag k=\"leisure\" v=\"park\" />\n",
    "\t\t<tag k=\"gnis:created\" v=\"04/06/1998\" />\n",
    "\t\t<tag k=\"gnis:state_id\" v=\"06\" />\n",
    "\t\t<tag k=\"gnis:county_id\" v=\"081\" />\n",
    "\t\t<tag k=\"gnis:feature_id\" v=\"1785701\" />\n",
    "\t</node>\n",
    "\n",
    "### For Node:\n",
    "\n",
    "The dictionary returns the format {\"node\": .., \"node_tags\": ...}\n",
    "\n",
    "The \"node\" field holds a dictionary of the following top level node attributes:\n",
    "\n",
    "id, user, uid, version, lat, lon, timestamp, changeset (All other attributes are ignored)\n",
    "\n",
    "The \"node_tags\" field holds a list of dictionaries, one per secondary tag. Secondary tags are child tags of node which have the tag name/type: \"tag\". Each dictionary has the following fields from the secondary tag attributes:\n",
    "\n",
    "- id: the top level node id attribute value \n",
    "    - This will be node['id'] = '358830414' in the above example\n",
    "- key: the full tag \"k\" attribute value if no colon is present or the characters after the colon if one is.\n",
    "    - k=\"name\" is one example \n",
    "- value: the tag \"v\" attribute value\n",
    "    - v=\"Longview Park\" is the value for the key k=\"name\"\n",
    "- type: either the characters before the colon in the tag \"k\" value or \"regular\" if a colon is not present.\n",
    "    - For k=\"name\", the type would be 'regular'\n",
    "- if the tag \"k\" value contains problematic characters, the tag should be ignored\n",
    "- if the tag \"k\" value contains a \":\" the characters before the \":\" should be set as the tag type and characters after the \":\" should be set as the tag key\n",
    "    - In < tag k=\"gnis:county_id\" > since a ':' is present, the tag['type'] = 'gnis' and tag['key'] = 'county_id'\n",
    "- if there are additional \":\" in the \"k\" value they and they should be ignored and kept as part of the tag key. For example should be turned into:\n",
    "    - {'id': 12345, 'key': 'street:name', 'value': 'Lincoln', 'type': 'addr'}\n",
    "- If a node has no secondary tags then the \"node_tags\" field should just contain an empty list.\n",
    "\n",
    "### For Way:\n",
    "\n",
    "The dictionary has the format {\"way\": ..., \"way_tags\": ..., \"way_nodes\": ...}\n",
    "\n",
    "The \"way\" field should hold a dictionary of the following top level way attributes:\n",
    "\n",
    "id, user, uid, version, timestamp, changeset (All other attributes are ignored)\n",
    "\n",
    "The \"way_tags\" field again holds a list of dictionaries, following the exact same rules as for \"node_tags\".\n",
    "Additionally, the dictionary has a field \"way_nodes\". \"way_nodes\" holds a list of dictionaries, one for each nd child tag. Each dictionary has the fields:\n",
    "\n",
    "- id: the top level element (way) id\n",
    "- node_id: the ref attribute value of the nd tag\n",
    "- position: the index starting at 0 of the nd tag i.e. what order the nd tag appears within the way element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "======================================================================================================================\n",
    "To write the data into CSV files, I defined the 'shape_element' function where I will also use my 'update_name' and 'update_postcode' functions to clean the street names and postcodes before they are inserted into the CSV files.\n",
    "\n",
    "In my shaping_csv.py file where the shape_element function is located, I import the audit.py file which contains the update_name and update_postcode functions. How I import the audit.py script is:\n",
    "- Have audit.py and shaping_csv.py files in the same directory\n",
    "- In shaping_csv.py script, use the following command:\n",
    "    - from audit import *   \n",
    "- The above command imports all the functions from the audit.py script into the shaping_csv.py script\n",
    "\n",
    "I call update_name and update_postcode functions twice in the code; once for the node element and one for the way element. The place to call them is when I am iterating through the 'tag' element for node or way, and I reach the attrib['v'].\n",
    "\n",
    "For tag['key'] and tag['type'], I used regular expressions to process all types of them. If there is a colon ':' in the tag['key'], I wrote the following patterns:\n",
    "\n",
    "- Characters before colon -> ^[a-zA-Z]*:\n",
    "    - This pattern starts from the beginning of the string, and matches all letters from a-z until it reaches a colon.\n",
    "    - This pattern is used to define the ['type'] \n",
    "\n",
    "- Characters after colon -> :[a-zA-Z_]+\n",
    "    - This pattern starts from the first time it finds a colon ':' and continue with the rest of the string.\n",
    "    - This pattern also makes sure to catch strings with an underscore '_'. For example in k=\"gnis:county_id , the pattern sets the ['key'] = county_id, not county.\n",
    "    - This pattern is used to define the ['key']\n",
    "\n",
    "One issue I noticed while I was validating my CSV file against the expected schema, I found out that some 'uid' values are missing from the data. Although according to the best practice of OpenStreetMap, all user's information like (uid and user fields) should be written while submitting data, they are not; thus, causing validation to throw errors.\n",
    "\n",
    "To fix this issue, I added a try and except statement, and gave a fake value to that attribute.  \n",
    "        \n",
    "        try:       \n",
    "            way_attribs[item] = element.attrib[item]  \n",
    "        except:        \n",
    "            way_attribs[item] = \"9999999\"  \n",
    "            \n",
    "The same problem with some empty fields are relatable to the 'k' attribute as well; meaning users did not added information for all the attributes in node or way. To overcome this, I used a conditional statement, and set the ['type'] and ['key'] to 'regular' in case the field is empty. Else, it uses the regular expressions to find corresponding patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PROBLEMCHARS = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "def shape_element(element):\n",
    "\n",
    "    node_attribs = {} # Handle the attributes in node element\n",
    "    way_attribs = {} # Handle the attributes in way element\n",
    "    way_nodes = [] # Handle the 'nd' tag in the way element\n",
    "    tags = []  # Handle secondary tags the same way for both node and way elements\n",
    "    \n",
    "    # Handling node elements\n",
    "    if element.tag == 'node':\n",
    "        for item in NODE_FIELDS:\n",
    "            try:\n",
    "                node_attribs[item] = element.attrib[item]\n",
    "            except:\n",
    "                node_attribs[item] = \"9999999\"\n",
    "        \n",
    "        # Iterating through the 'tag' tags in the node element\n",
    "        for tg in element.iter('tag'):\n",
    "            if not PROBLEMCHARS.search(tg.attrib['k']):\n",
    "                tag_dict_node = {}\n",
    "                tag_dict_node['id'] = element.attrib['id']\n",
    "\n",
    "                # Calling the update_name function to clean up problematic street names based on audit.py file\n",
    "                if is_street_name(tg):\n",
    "                    better_name = update_name(tg.attrib['v'], mapping)\n",
    "                    tag_dict_node['value'] = better_name\n",
    "\n",
    "                # Calling the update_postcode function to clean up problematic postcodes based on audit.py file\n",
    "                elif get_postcode(tg):\n",
    "                    better_postcode = update_postcode(tg.attrib['v'])\n",
    "                    tag_dict_node['value'] = better_postcode\n",
    "                \n",
    "                # For other values that are not street names or postcodes\n",
    "                else:\n",
    "                    tag_dict_node['value'] = tg.attrib['v']\n",
    "\n",
    "                if ':' not in tg.attrib['k']:\n",
    "                    tag_dict_node['key'] = tg.attrib['k']\n",
    "                    tag_dict_node['type'] = 'regular'\n",
    "                else: \n",
    "                    character_before_colon = re.findall('^[a-zA-Z]*:', tg.attrib['k'])\n",
    "                    character_after_colon = re.findall(':[a-zA-Z_]+' , tg.attrib['k'])\n",
    "                    if len(character_after_colon) != 0:\n",
    "                        tag_dict_node['key'] = character_after_colon[0][1:]\n",
    "                    else:\n",
    "                        tag_dict_node['key'] = 'regular'\n",
    "\n",
    "                    if len(character_before_colon) != 0:\n",
    "                        tag_dict_node['type'] = character_before_colon[0][: -1]\n",
    "                    else:\n",
    "                        tag_dict_node['type'] = 'regular'\n",
    "                tags.append(tag_dict_node)\n",
    "            \n",
    "        return {'node': node_attribs, 'node_tags': tags}\n",
    "        \n",
    "    # Handling way elements\n",
    "    elif element.tag == 'way':\n",
    "        for item in WAY_FIELDS:\n",
    "            try:\n",
    "                way_attribs[item] = element.attrib[item]\n",
    "            except:\n",
    "                way_attribs[item] = \"9999999\"\n",
    "        \n",
    "        # Iterating through 'tag' tags in way element\n",
    "        for tg in element.iter('tag'):\n",
    "            if not PROBLEMCHARS.search(tg.attrib['k']):\n",
    "                tag_dict_way = {}\n",
    "                tag_dict_way['id'] = element.attrib['id']\n",
    "\n",
    "                # Calling the update_name function to clean up problematic street names based on audit.py file\n",
    "                if is_street_name(tg):\n",
    "                    better_name_way = update_name(tg.attrib['v'], mapping)\n",
    "                    tag_dict_way['value'] = better_name_way\n",
    "\n",
    "                # Calling the update_postcode function to clean up problematic postcodes based on audit.py file\n",
    "                if get_postcode(tg):\n",
    "                    better_postcode_way = update_postcode(tg.attrib['v'])\n",
    "                    tag_dict_way['value'] = better_postcode_way\n",
    "\n",
    "                # For other values that are not street names or postcodes\n",
    "                else:\n",
    "                    tag_dict_way['value'] = tg.attrib['v']\n",
    "\n",
    "                if ':' not in tg.attrib['k']:\n",
    "                    tag_dict_way['key'] = tg.attrib['k']\n",
    "                    tag_dict_way['type'] = 'regular'\n",
    "                else:\n",
    "                    character_before_colon = re.findall('^[a-zA-Z]*:', tg.attrib['k'])\n",
    "                    character_after_colon = re.findall(':[a-zA-Z_]+', tg.attrib['k'])\n",
    "                \n",
    "                    if len(character_after_colon) == 1:\n",
    "                        tag_dict_way['key'] = character_after_colon[0][1:]\n",
    "                    if len(character_after_colon) > 1:\n",
    "                        tag_dict_way['key'] = character_after_colon[0][1: ] + character_after_colon[1]\n",
    "                \n",
    "                    if len(character_before_colon) != 0:\n",
    "                        tag_dict_way['type'] = character_before_colon[0][: -1]\n",
    "                    else:\n",
    "                        tag_dict_way['type'] = 'regular'\n",
    "                \n",
    "                tags.append(tag_dict_way)\n",
    "        \n",
    "        # Iterating through 'nd' tags in way element\n",
    "        count = 0\n",
    "        for tg in element.iter('nd'):\n",
    "            tag_dict_nd = {}\n",
    "            tag_dict_nd['id'] = element.attrib['id']\n",
    "            tag_dict_nd['node_id'] = tg.attrib['ref']\n",
    "            tag_dict_nd['position'] = count\n",
    "            count += 1\n",
    "            \n",
    "            way_nodes.append(tag_dict_nd)\n",
    "        \n",
    "        return {'way': way_attribs, 'way_nodes': way_nodes, 'way_tags': tags}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the shape_element function in place, I can now parse and shape the data, and write it to CSV files.\n",
    "\n",
    "The main function is what I used to call my audit function to update street names and postcodes. The python script shaping_csv.py takes care of creating the CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OSM_PATH = '/Users/nazaninmirarab/Desktop/Data Science/P3/Project/Submission2/san-francisco_california_sample.osm'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    process_map(OSM_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the CSV files are created (by running the shaping_csv.py file), it is time to create the database and insert the information from those CSV files to their corresponding tables.\n",
    "\n",
    "I created the database called 'openstreetmap_sf_db' and I created tables with columns based on the columns from the CSV files, and inserted the data from the CSV files to the corresponding tables in the database. The 'creating_database.py' file takes care of creating the tables and inserting data in them.\n",
    "\n",
    "After the tables are created, I can now start investigating them and getting queries on them.\n",
    "\n",
    "The code below shows how I have created the tables in the the database. This is an example code where tables_name, column_name and filename.csv are replaced according to the table I want to insert in the database.\n",
    "\n",
    "First I connect to the sqlite file, and make sure to check that the table I want to create is not already created.\n",
    "Using 'cur.execute' I execute commands to the database in python. After creating the table, I inserted the data from the CSV file into it. I did this process for every table I wanted to create in the database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import csv\n",
    "from pprint import pprint\n",
    "\n",
    "sqlite_file = 'db.sqlite'\n",
    "\n",
    "conn = sqlite3.connect(sqlite_file)\n",
    "cur = conn.cursor()\n",
    "\n",
    "#making sure a table that already exists does not get created\n",
    "cur.execute('DROP TABLE IF EXISTS nodes')\n",
    "conn.commit()\n",
    "\n",
    "#creating the table\n",
    "cur.execute('''\n",
    "    CREATE TABLE tables_name(column_name type, column_name type, column_name type, column_name type)\n",
    "''')\n",
    "conn.commit()\n",
    "\n",
    "with open('filename.csv', 'rb') as f:\n",
    "    dr = csv.DictReader(f)\n",
    "    in_db = [(i['column_name'], i['column_name'], i['column_name'], i['column_name']) for i in dr]\n",
    "    \n",
    "#insert the data\n",
    "cur.executemany('INSERT INTO tables_name(column_name, column_name, column_name, column_name) VALUES(?, ?, ?, ?);', in_db)\n",
    "conn.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to get some information regarding the CSV files and the database I created.\n",
    "\n",
    "By importing 'hurry.filesize' I can translate the file sizes from bytes to KB or MB. To install the library, you need to 'pip install hurry.filesize' it on your machine. I got the idea from using this method from the post below:  \n",
    "https://discussions.udacity.com/t/display-files-and-their-sizes-in-directory/186741"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nodes.csv...............................: 378M \n",
      "nodes_tags.csv..........................: 8M   \n",
      "openstreetmap_sf_db.sqlite..............: 514M \n",
      "san-francisco_california.osm............: 966M \n",
      "san-francisco_california_sample.osm.....: 48M  \n",
      "ways.csv................................: 31M  \n",
      "ways_nodes.csv..........................: 128M \n",
      "ways_tags.csv...........................: 48M  \n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "import os\n",
    "from hurry.filesize import size \n",
    "\n",
    "dirpath = '/Users/nazaninmirarab/Desktop/Data Science/P3/Project/Submission2/Sizes'\n",
    "\n",
    "files_list = []\n",
    "for path, dirs, files in os.walk(dirpath):\n",
    "    files_list.extend([(filename, size(os.path.getsize(os.path.join(path, filename)))) for filename in files])\n",
    "\n",
    "for filename, size in files_list:\n",
    "    print '{:.<40s}: {:5s}'.format(filename,size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have audited and cleaned the data and transfered everything into table in my database, I can start running queries on it. The queries answer many questions such as:   \n",
    "- Number of nodes\n",
    "- Number of way\n",
    "- Number of unique users\n",
    "- Most contributing users\n",
    "- Number of users who contributed only once\n",
    "- Top 10 amenities in San Fracisco\n",
    "- Cuisines in San Francisco\n",
    "- Shops in San Francisco\n",
    "- Users who added amenities \n",
    "\n",
    "This is basically when the fun with the data starts. You can go through it and extract as much information as you like. You just need to come up with a question, and write your query to find its answer!\n",
    "\n",
    "First, I make sure I am connected to the database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "sqlite_file = '/Users/nazaninmirarab/Desktop/Data Science/P3/Project/Submission2/openstreetmap_sf_db.sqlite'\n",
    "con = sqlite3.connect(sqlite_file)\n",
    "cur = con.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: \n",
      "4714877\n"
     ]
    }
   ],
   "source": [
    "def number_of_nodes():\n",
    "    output = cur.execute('SELECT COUNT(*) FROM nodes')\n",
    "    return output.fetchone()[0]\n",
    "\n",
    "print 'Number of nodes: \\n' , number_of_nodes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ways: \n",
      "551145\n"
     ]
    }
   ],
   "source": [
    "def number_of_ways():\n",
    "    output = cur.execute('SELECT COUNT(*) FROM ways')\n",
    "    return output.fetchone()[0]\n",
    "\n",
    "print 'Number of ways: \\n' , number_of_ways()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of unique users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique users: \n",
      "2579\n"
     ]
    }
   ],
   "source": [
    "def number_of_unique_users():\n",
    "    output = cur.execute('SELECT COUNT(DISTINCT e.uid) FROM \\\n",
    "                         (SELECT uid FROM nodes UNION ALL SELECT uid FROM ways) e')\n",
    "    return output.fetchone()[0]\n",
    "\n",
    "print 'Number of unique users: \\n' , number_of_unique_users()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most contributing users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most contributing users: \n",
      "\n",
      "[(u'ediyes', 918915),\n",
      " (u'Luis36995', 710456),\n",
      " (u'Rub21', 395077),\n",
      " (u'RichRico', 224724),\n",
      " (u'calfarome', 185498),\n",
      " (u'oldtopos', 167538),\n",
      " (u'KindredCoda', 151208),\n",
      " (u'karitotp', 135330),\n",
      " (u'samely', 125861),\n",
      " (u'abel801', 108313)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def most_contributing_users():\n",
    "    \n",
    "    output = cur.execute('SELECT e.user, COUNT(*) as num FROM \\\n",
    "                         (SELECT user FROM nodes UNION ALL SELECT user FROM ways) e \\\n",
    "                         GROUP BY e.user \\\n",
    "                         ORDER BY num DESC \\\n",
    "                         LIMIT 10 ')\n",
    "    pprint(output.fetchall())\n",
    "    return output.fetchall()\n",
    "\n",
    "print 'Most contributing users: \\n'\n",
    "most_contributing_users()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of users who contributed once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users who have contributed once: \n",
      "634\n"
     ]
    }
   ],
   "source": [
    "def number_of_users_contributed_once():\n",
    "    \n",
    "    output = cur.execute('SELECT COUNT(*) FROM \\\n",
    "                             (SELECT e.user, COUNT(*) as num FROM \\\n",
    "                                 (SELECT user FROM nodes UNION ALL SELECT user FROM ways) e \\\n",
    "                                  GROUP BY e.user \\\n",
    "                                  HAVING num = 1) u')\n",
    "    \n",
    "    return output.fetchone()[0]\n",
    "                         \n",
    "print 'Number of users who have contributed once: \\n', number_of_users_contributed_once()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 amenities in San Francisco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top ten amenities: \n",
      "\n",
      "[(u'restaurant', 2816),\n",
      " (u'bench', 1137),\n",
      " (u'cafe', 943),\n",
      " (u'place_of_worship', 719),\n",
      " (u'post_box', 680),\n",
      " (u'school', 604),\n",
      " (u'fast_food', 562),\n",
      " (u'bicycle_parking', 556),\n",
      " (u'drinking_water', 492),\n",
      " (u'toilets', 394),\n",
      " (u'bank', 364),\n",
      " (u'bar', 314),\n",
      " (u'parking', 272),\n",
      " (u'fuel', 270),\n",
      " (u'car_sharing', 225),\n",
      " (u'waste_basket', 203),\n",
      " (u'pub', 200),\n",
      " (u'atm', 189),\n",
      " (u'post_office', 156),\n",
      " (u'pharmacy', 143)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def top_ten_amenities_in_sf():\n",
    "    output = cur.execute('SELECT value, COUNT(*) as num FROM nodes_tags\\\n",
    "                            WHERE key=\"amenity\" \\\n",
    "                            GROUP BY value \\\n",
    "                            ORDER BY num DESC \\\n",
    "                            LIMIT 20' )\n",
    "    pprint(output.fetchall())\n",
    "    return output.fetchall()\n",
    "\n",
    "print 'Top ten amenities: \\n'\n",
    "top_ten_amenities_in_sf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 cuisines in San Francisco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 cuisines: \n",
      "\n",
      "[(u'burger', 71),\n",
      " (u'mexican', 47),\n",
      " (u'pizza', 29),\n",
      " (u'chinese', 26),\n",
      " (u'american', 20),\n",
      " (u'coffee_shop', 19),\n",
      " (u'italian', 15),\n",
      " (u'japanese', 15),\n",
      " (u'sushi', 12),\n",
      " (u'seafood', 10)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cuisines_in_sf():\n",
    "    output = cur.execute ('SELECT value, COUNT(*) as num FROM ways_tags \\\n",
    "                           WHERE key=\"cuisine\" \\\n",
    "                           GROUP BY value \\\n",
    "                           ORDER BY num DESC \\\n",
    "                           LIMIT 10')\n",
    "    pprint(output.fetchall())\n",
    "    return output.fetchall()\n",
    "\n",
    "print 'Top 10 cuisines: \\n'\n",
    "cuisines_in_sf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different types of shops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different types of shops: \n",
      "\n",
      "[(u'restaurant', 2816),\n",
      " (u'bench', 1137),\n",
      " (u'cafe', 943),\n",
      " (u'place_of_worship', 719),\n",
      " (u'post_box', 680),\n",
      " (u'school', 604),\n",
      " (u'fast_food', 562),\n",
      " (u'bicycle_parking', 556),\n",
      " (u'drinking_water', 492),\n",
      " (u'toilets', 394),\n",
      " (u'bank', 364),\n",
      " (u'bar', 314),\n",
      " (u'parking', 272),\n",
      " (u'fuel', 270),\n",
      " (u'car_sharing', 225),\n",
      " (u'waste_basket', 203),\n",
      " (u'pub', 200),\n",
      " (u'atm', 189),\n",
      " (u'post_office', 156),\n",
      " (u'pharmacy', 143)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def shops_in_sf():\n",
    "    output = cur.execute('SELECT value, COUNT(*) as num FROM nodes_tags\\\n",
    "                            WHERE key=\"shop\" \\\n",
    "                            GROUP BY value \\\n",
    "                            ORDER BY num DESC' )\n",
    "    pprint.pprint(output.fetchall())\n",
    "    return output.fetchall()\n",
    "\n",
    "print 'Different types of shops: \\n'\n",
    "top_ten_amenities_in_sf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Users who added amenities to the map\n",
    "\n",
    "Since the list is long and would make the look of this document not nice, I added the limit of showing only 10 of them. The way to show those users is by no order. If 'LIMIT 10' is removed, you can view the whole list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users who added amenity to the map: \n",
      "\n",
      "[(u'claysmalley', u'Corner Market'),\n",
      " (u'dchiles', u'Note 281478'),\n",
      " (u'lxbarth', u'Pet grooming shop'),\n",
      " (u'oldtopos', u'addr:housenumber'),\n",
      " (u'JessAk71', u'amusements'),\n",
      " (u'Mark Mavromatis', u'animal_shelter'),\n",
      " (u'Jothirnadh', u'arts_centre'),\n",
      " (u'manings', u'atm'),\n",
      " (u'JessAk71', u'bakery'),\n",
      " (u'poornibadrinath', u'bank')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def users_who_added_amenity():\n",
    "    output = cur.execute('SELECT DISTINCT(nodes.user), nodes_tags.value FROM \\\n",
    "                            nodes join nodes_tags \\\n",
    "                            on nodes.id=nodes_tags.id \\\n",
    "                            WHERE key=\"amenity\" \\\n",
    "                            GROUP BY value \\\n",
    "                            LIMIT 10' ) # Remove this part to view the whole list of users\n",
    "    pprint(output.fetchall())\n",
    "    return output.fetchall()\n",
    "\n",
    "print 'Users who added amenity to the map: \\n'\n",
    "users_who_added_amenity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of postcodes\n",
    "\n",
    "Since I did some cleaning on the varieties of postcode format, I want to get a query from them and see how they look like now. While I was audting and cleaning the postcode formats that were either 6-digit long, shorter than 5-digit long or equaled to only 'CA', I decided that since they were not valid, I could set them to zero. \n",
    "\n",
    "By running the below query on the database after applying the changes, I now can see that there are 10 postcodes set to zero:   \n",
    "\n",
    "        (u'00000', 10)\n",
    "        \n",
    "For the purpose of this documentation, I have only printed the top-5. If you remove the 'LIMIT 5' part from the query, you will be able to see the complete list of postcodes.\n",
    "\n",
    "We also see that the most repetative postcode is 94122 with the highest number of 5109 repetitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of postcodes: \n",
      "\n",
      "[(u'94122', 5109),\n",
      " (u'94611', 2990),\n",
      " (u'94116', 2202),\n",
      " (u'94610', 1357),\n",
      " (u'94117', 1220)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def list_of_postcodes():\n",
    "    output = cur.execute('SELECT e.value, COUNT(*) as num FROM \\\n",
    "                            (SELECT value FROM nodes_tags WHERE key=\"postcode\"\\\n",
    "                             UNION ALL SELECT value FROM ways_tags WHERE key=\"postcode\") e \\\n",
    "                            GROUP BY e.value \\\n",
    "                            ORDER BY num DESC \\\n",
    "                            LIMIT 5' ) # Remove this limit to see the complete list of postcodes\n",
    "    pprint(output.fetchall())\n",
    "    return output.fetchall()\n",
    "\n",
    "print 'List of postcodes: \\n'\n",
    "list_of_postcodes()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amenities around 94122 Postcode\n",
    "\n",
    "Since 94122 is the most repepative postcode, I wanted to check to see what the amenities around this area are. Since the list was quite long, I limited it to the first 20 amenities with the highest number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amenities around 94122 postcode: \n",
      "\n",
      "[(u'restaurant', 2816),\n",
      " (u'bench', 1137),\n",
      " (u'cafe', 943),\n",
      " (u'place_of_worship', 719),\n",
      " (u'post_box', 680),\n",
      " (u'school', 604),\n",
      " (u'fast_food', 562),\n",
      " (u'bicycle_parking', 556),\n",
      " (u'drinking_water', 492),\n",
      " (u'toilets', 394),\n",
      " (u'bank', 364),\n",
      " (u'bar', 314),\n",
      " (u'parking', 272),\n",
      " (u'fuel', 270),\n",
      " (u'car_sharing', 225),\n",
      " (u'waste_basket', 203),\n",
      " (u'pub', 200),\n",
      " (u'atm', 189),\n",
      " (u'post_office', 156),\n",
      " (u'pharmacy', 143)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def amenities_around_94122():\n",
    "    output = cur.execute('SELECT nodes_tags.value, COUNT(*) as num \\\n",
    "                          FROM nodes_tags \\\n",
    "                            JOIN (SELECT DISTINCT(id) FROM nodes_tags WHERE key=\"amenity\") AS amenities \\\n",
    "                            ON nodes_tags.id = amenities.id \\\n",
    "                            WHERE nodes_tags.key=\"amenity\"\\\n",
    "                            GROUP BY nodes_tags.value \\\n",
    "                            ORDER BY num DESC \\\n",
    "                            LIMIT 20' ) # Remove this limit to see the complete list of postcodes\n",
    "    pprint(output.fetchall())\n",
    "    return output.fetchall()\n",
    "\n",
    "print 'Amenities around 94122 postcode: \\n'\n",
    "amenities_around_94122()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Popular Cafes in San Francisco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most popular cafes in San Francisco: \n",
      "\n",
      "[(u'Starbucks', 51),\n",
      " (u\"Peet's Coffee & Tea\", 15),\n",
      " (u'Starbucks Coffee', 15),\n",
      " (u\"Peet's Coffee and Tea\", 7),\n",
      " (u'Philz Coffee', 5),\n",
      " (u\"Peet's Coffee\", 4),\n",
      " (u'Beanery', 3),\n",
      " (u'Blue Bottle Coffee', 3),\n",
      " (u'Highwire Coffee Roasters', 3),\n",
      " (u'Alchemy Collective Cafe', 2)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def most_popular_cafes():\n",
    "    output = cur.execute('SELECT nodes_tags.value, COUNT(*) as num \\\n",
    "                          FROM nodes_tags \\\n",
    "                            JOIN (SELECT DISTINCT(id) FROM nodes_tags WHERE value=\"coffee_shop\") AS cafes \\\n",
    "                            ON nodes_tags.id = cafes.id \\\n",
    "                            WHERE nodes_tags.key=\"name\"\\\n",
    "                            GROUP BY nodes_tags.value \\\n",
    "                            ORDER BY num DESC \\\n",
    "                            LIMIT 10' ) # Remove this limit to see the complete list of postcodes\n",
    "    pprint(output.fetchall())\n",
    "    return output.fetchall()\n",
    "\n",
    "print 'Most popular cafes in San Francisco: \\n'\n",
    "most_popular_cafes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No surprise that Starbucks in the most popular brand in the US. There are, however, some mistakes in the names. For example 'Starbuck' and 'Starbucks Coffee' are the same thing with different names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussions about the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anticipated Problems\n",
    "\n",
    "Data wrangling for this project has been a time-consuming and comlicated task due to many inconsistencies in the data. I could spot only a few those problems and clean them up, but I am sure there are many that I could not catch. The main reason behind these inconsistencies is human error; however, there can be way to improve the overall quality of the data, especially when it is supposed to be used for means of analysis.\n",
    "\n",
    "#### Empty user id fields\n",
    "\n",
    "This was one of the problems I found out about while I was creating my CSV files and trying to validate them against the correct schema. I received the error that the 'uid' field is empty. This came as a surprise for me since I thought they least of what contributors can include is to not leave the 'uid' field empty. \n",
    "\n",
    "According to best practices of OpenStreetMap, the 'uid' is a part that needs to be added; however, there were many empty fields in the data for this item. OpenStreetMap can make this a mandatory step for contributors but on the other hand this might descrease the number of contributions.\n",
    "\n",
    "#### Invalid format for postcodes\n",
    "\n",
    "Postcode formats were one other issues I had to catch and fix a few times during this project. While aduting the postcodes in the sample file, I was able to spot some issues, but after running the script against the priginal file, I saw there were more problems that I had not seen. The standard postcode format was in form of a 5-digit code, with no letters or other characters; however, there were formats with the state abbreviation before the digit, which is still acceptables. But, formats where the code was 6-digit long or shorter than 5-digit long or only 'CA' were invalid, and in my opinion there needs to be some checks before the user can add the data to the map. For example, there can be easy checks for fields like postcode to make sure no invalid data is entered. This can also help contributors to know what is expected of them.\n",
    "\n",
    "### Suggestion for Improvement\n",
    "\n",
    "#### Gamification\n",
    "\n",
    "One way that could help to enhance the quality and accuracy of the data can be using gamification methods such as 'top contributors' to increase the level of motivation for submitting more data. This gamification, though, needs to apply OpenStreetMap best practices so that the data submitted has less inconsistencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
